{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, mode, image_size, num_classes):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # Define various layers here, such as in the tutorial example\n",
    "        # self.conv1 = nn.Conv2D(...)\n",
    "        self.conv1 = nn.Conv2d(\n",
    "            in_channels=1,\n",
    "            out_channels=40,\n",
    "            kernel_size=(5, 5),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "        )\n",
    "        self.pool = nn.MaxPool2d(kernel_size=(2, 2), stride=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=40,\n",
    "            out_channels=40,\n",
    "            kernel_size=(5, 5),\n",
    "            stride=(1, 1),\n",
    "            padding=(1, 1),\n",
    "        )\n",
    "        \n",
    "        if mode == 1:\n",
    "            self.fc1 = nn.Linear(image_size, 100)\n",
    "            self.fc2 = nn.Linear(100, num_classes)\n",
    "        if mode == 2 or mode == 3:\n",
    "            self.fc1 = nn.Linear(19360, 100)\n",
    "            self.fc2 = nn.Linear(100, num_classes)\n",
    "        if mode == 4:\n",
    "            self.fc1 = nn.Linear(23 * 23 * 40, 100)\n",
    "            self.fc2 = nn.Linear(100, 100)\n",
    "            self.fc3 = nn.Linear(100, num_classes)\n",
    "        if mode == 5:\n",
    "            self.fc1 = nn.Linear(23 * 23 * 40, 1000)\n",
    "            self.fc2 = nn.Linear(1000, 1000)\n",
    "            self.fc3 = nn.Linear(1000, num_classes)\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # This will select the forward pass function based on mode for the ConvNet.\n",
    "        # Based on the question, you have 5 modes available for step 1 to 5.\n",
    "        # During creation of each ConvNet model, you will assign one of the valid mode.\n",
    "        # This will fix the forward function (and the network graph) for the entire training/testing\n",
    "        if mode == 1:\n",
    "            self.forward = self.model_1\n",
    "        elif mode == 2:\n",
    "            self.forward = self.model_2\n",
    "        elif mode == 3:\n",
    "            self.forward = self.model_3\n",
    "        elif mode == 4:\n",
    "            self.forward = self.model_4\n",
    "        elif mode == 5:\n",
    "            self.forward = self.model_5\n",
    "        else: \n",
    "            print(\"Invalid mode \", mode, \"selected. Select between 1-5\")\n",
    "            exit(0)\n",
    "        \n",
    "        \n",
    "    # Baseline model. step 1\n",
    "    def model_1(self, X):\n",
    "        # ======================================================================\n",
    "        # One fully connected layer. STEP 1: Create a fully connected (FC) hidden layer (with 100 neurons) with Sigmoid activation function.\n",
    "        # Train it with SGD with a learning rate of 0.1 (a total of 60 epoch), a mini-batch size of 10, and no regularization.\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        X = torch.sigmoid(self.fc1(X))\n",
    "        X = self.fc2(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "    # Use two convolutional layers.\n",
    "    def model_2(self, X):\n",
    "        # ======================================================================\n",
    "        # Two convolutional layers + one fully connnected layer.\n",
    "        X = torch.sigmoid(self.conv1(X))\n",
    "        X = self.pool(X)\n",
    "        X = torch.sigmoid(self.conv2(X))\n",
    "        X = self.pool(X)\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        X = torch.sigmoid(self.fc1(X))\n",
    "        X = self.fc2(X)\n",
    "        \n",
    "        return X\n",
    "\n",
    "    # Replace sigmoid with ReLU.\n",
    "    def model_3(self, X):\n",
    "        # ======================================================================\n",
    "        # Two convolutional layers + one fully connected layer, with ReLU.\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = self.pool(X)\n",
    "        X = F.relu(self.conv2(X))\n",
    "        X = self.pool(X)\n",
    "        X = X.reshape(X.shape[0], -1)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = self.fc2(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "    # Add one extra fully connected layer.\n",
    "    def model_4(self, X):\n",
    "        # ======================================================================\n",
    "        # Two convolutional layers + two fully connected layers, with ReLU.\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = self.pool(X)\n",
    "        x = F.relu(self.conv2(X))\n",
    "        X = self.pool(X)\n",
    "        X = x.reshape(X.shape[0], -1)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.fc3(X)\n",
    "\n",
    "        return X\n",
    "\n",
    "    # Use Dropout now.\n",
    "    def model_5(self, X):\n",
    "        # ======================================================================\n",
    "        # Two convolutional layers + two fully connected layers, with ReLU.\n",
    "        # and  + Dropout.\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = self.pool(X)\n",
    "        x = F.relu(self.conv2(X))\n",
    "        X = self.pool(X)\n",
    "        X = x.reshape(X.shape[0], -1)\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = self.dropout(X)\n",
    "        X = self.fc3(X)\n",
    "\n",
    "        return X\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# from ConvNet import ConvNet \n",
    "import argparse\n",
    "import numpy as np     \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch device selected:  cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if cuda is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Set proper device based on cuda availability \n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(\"Torch device selected: \", device)\n",
    "\n",
    "# Create transformations to apply to each data sample \n",
    "# Can specify variations such as image flip, color flip, random crop, ...\n",
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "\n",
    "# Load datasets for training and testing\n",
    "# Inbuilt datasets available in torchvision (check documentation online)\n",
    "dataset1 = datasets.MNIST('./data/', train=True, download=True,\n",
    "                    transform=transform)\n",
    "dataset2 = datasets.MNIST('./data/', train=False,\n",
    "                    transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(batch_size=10, num_workers=4):\n",
    "    train_loader = DataLoader(dataset1, batch_size = batch_size, \n",
    "                            shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(dataset2, batch_size = batch_size, \n",
    "                                shuffle=False, num_workers=4)\n",
    "    \n",
    "    return train_loader, test_loader\n",
    "\n",
    "def plot(num_epochs, train_losses, train_accuracies, save=0, mode=1):\n",
    "    x = range(1, num_epochs+1)\n",
    "\n",
    "    plt.plot(x, train_losses)\n",
    "    plt.plot(x, train_accuracies)\n",
    "    plt.legend(['Train Loss', 'Train Accuracy'])\n",
    "    \n",
    "    if save:\n",
    "        plt.savefig(f'plots/model_{mode}.jpg')\n",
    "        plt.show()\n",
    "\n",
    "def train(model, device, train_loader, optimizer, criterion, epoch, batch_size, num_epochs):\n",
    "    '''\n",
    "    Trains the model for an epoch and optimizes it.\n",
    "    model: The model to train. Should already be in correct device.\n",
    "    device: 'cuda' or 'cpu'.\n",
    "    train_loader: dataloader for training samples.\n",
    "    optimizer: optimizer to use for model parameter updates.\n",
    "    criterion: used to compute loss for prediction and target \n",
    "    epoch: Current epoch to train for.\n",
    "    batch_size: Batch size to be used.\n",
    "    '''\n",
    "    \n",
    "    # Set model to train mode before each epoch\n",
    "    model.train()\n",
    "    \n",
    "    # Empty list to store losses \n",
    "    losses = []\n",
    "    correct = 0\n",
    "    # Iterate over entire training samples (1 epoch)\n",
    "    for batch_idx, batch_sample in enumerate(train_loader):\n",
    "        data, target = batch_sample\n",
    "        # print(f'{data.shape = }')\n",
    "        \n",
    "        # Push data/label to correct device\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # Reset optimizer gradients. Avoids grad accumulation (accumulation used in RNN).\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Do forward pass for current set of data\n",
    "        output = model(data)\n",
    "        # Compute loss based on criterion\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        # Computes gradient based on final loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # Store loss\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Optimize model parameters based on learning rate and gradient \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Get predicted index by selecting maximum log-probability\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        \n",
    "        _, predictions = output.max(1)\n",
    "        correct += (predictions == target).sum()\n",
    "        print(f'Training epoch: ({epoch}/{num_epochs}) batch: ({batch_idx+1}/{len(train_loader)})', end='\\r') #. Acc: {correct}/{(batch_idx+1) * batch_size}, {100. * correct / ((batch_idx+1) * batch_size)}', end='\\r')\n",
    "        \n",
    "    train_loss = float(np.mean(losses))\n",
    "    train_acc = correct / ((batch_idx+1) * batch_size)\n",
    "    print('\\nTrain set ({}/{}): Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(epoch, num_epochs,\n",
    "        float(np.mean(losses)), correct, (batch_idx+1) * batch_size,\n",
    "        100. * correct / ((batch_idx+1) * batch_size)))\n",
    "    return train_loss, train_acc\n",
    "    \n",
    "def test(model, device, test_loader, criterion, epoch, num_epochs, batch_size):\n",
    "    '''\n",
    "    Tests the model.\n",
    "    model: The model to train. Should already be in correct device.\n",
    "    device: 'cuda' or 'cpu'.\n",
    "    test_loader: dataloader for test samples.\n",
    "    '''\n",
    "    \n",
    "    # Set model to eval mode to notify all layers.\n",
    "    model.eval()\n",
    "    \n",
    "    losses = []\n",
    "    correct = 0\n",
    "    \n",
    "    # Set torch.no_grad() to disable gradient computation and backpropagation\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, sample in enumerate(test_loader):\n",
    "            data, target = sample\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            # Predict for data by doing forward pass\n",
    "            output = model(data)\n",
    "        \n",
    "            # Compute loss based on same criterion as training \n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # Append loss to overall test loss\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Get predicted index by selecting maximum log-probability\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            \n",
    "            _, predictions = output.max(1)\n",
    "            correct += (predictions == target).sum()\n",
    "            print(f'Testing epoch: ({epoch}/{num_epochs}) batch: ({batch_idx+1}/{len(test_loader)})', end='\\r')\n",
    "\n",
    "    test_loss = float(np.mean(losses))\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set ({}/{}): Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(epoch, num_epochs,\n",
    "        test_loss, correct, len(test_loader.dataset), accuracy))\n",
    "    \n",
    "    return test_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(mode=1, learning_rate=0.01, batch_size=10, num_epochs=60):\n",
    "    image_size = 28*28\n",
    "    num_classes = 10\n",
    "\n",
    "    # Initialize the model and send to device \n",
    "    model = ConvNet(mode, image_size, num_classes).to(device)\n",
    "    # Define loss function.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Define optimizer function.\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    # Define data loaders\n",
    "    train_loader, test_loader = load_data(batch_size)\n",
    "\n",
    "    best_accuracy = 0.0\n",
    "\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    # Run training for n_epochs specified in config \n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_loss, train_accuracy = train(model, device, train_loader,\n",
    "                                            optimizer, criterion, epoch, batch_size, num_epochs)\n",
    "        test_loss, test_accuracy = test(model, device, test_loader, criterion, epoch, num_epochs, batch_size)\n",
    "        \n",
    "        if test_accuracy > best_accuracy:\n",
    "            best_accuracy = test_accuracy\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy.cpu().numpy())\n",
    "\n",
    "    plot(num_epochs, train_losses, train_accuracies, save=1, mode=mode)\n",
    "\n",
    "    print(\"Accuracy: {:2.2f}%\".format(best_accuracy))\n",
    "\n",
    "    print(\"Training and evaluation finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================ Training model 1 ================================\n",
      "A fully connected (FC) hidden layer (with 100 neurons) with Sigmoid activation function.\n",
      "\n",
      "learning_rate = 0.1\n",
      "batch_size = 10\n",
      "num_epochs = 60\n",
      "\n",
      "Training epoch: (1/60) batch: (1743/6000)\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/d_/ck6qtb554x7fcmpxs24wxn1w0000gn/T/ipykernel_51777/3478335711.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A fully connected (FC) hidden layer (with 100 neurons) with Sigmoid activation function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nlearning_rate = {}\\nbatch_size = {}\\nnum_epochs = {}\\n'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrun_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'='\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/d_/ck6qtb554x7fcmpxs24wxn1w0000gn/T/ipykernel_51777/185664921.py\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(mode, learning_rate, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;31m# Run training for n_epochs specified in config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         train_loss, train_accuracy = train(model, device, train_loader,\n\u001b[0m\u001b[1;32m     21\u001b[0m                                             optimizer, criterion, epoch, batch_size, num_epochs)\n\u001b[1;32m     22\u001b[0m         \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/d_/ck6qtb554x7fcmpxs24wxn1w0000gn/T/ipykernel_51777/831085847.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, criterion, epoch, batch_size, num_epochs)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;31m# Optimize model parameters based on learning rate and gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;31m# Get predicted index by selecting maximum log-probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    144\u001b[0m                         \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'momentum_buffer'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             sgd(params_with_grad,\n\u001b[0m\u001b[1;32m    147\u001b[0m                 \u001b[0md_p_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36msgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, has_sparse_grad, foreach, weight_decay, momentum, lr, dampening, nesterov, maximize)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_sgd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     func(params,\n\u001b[0m\u001b[1;32m    198\u001b[0m          \u001b[0md_p_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m          \u001b[0mmomentum_buffer_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36m_single_tensor_sgd\u001b[0;34m(params, d_p_list, momentum_buffer_list, weight_decay, momentum, lr, dampening, nesterov, maximize, has_sparse_grad)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m         \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmaximize\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ================== Model 1 ==================\n",
    "learning_rate = 0.1\n",
    "batch_size = 10\n",
    "num_epochs = 60\n",
    "\n",
    "print('\\n\\n'+('='*32)+' Training model 1 '+('='*32))\n",
    "print('A fully connected (FC) hidden layer (with 100 neurons) with Sigmoid activation function.')\n",
    "print('\\nlearning_rate = {}\\nbatch_size = {}\\nnum_epochs = {}\\n'.format(learning_rate, batch_size, num_epochs))\n",
    "run_model(mode=1, learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epochs)    \n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Model 2 ==================\n",
    "learning_rate = 0.1\n",
    "batch_size = 10\n",
    "num_epochs = 60\n",
    "\n",
    "print(('='*32)+' Training model 2 '+('='*32))\n",
    "print('Model 1 + two convolutional layer that pool over 2x2 regions, 40 kernels, stride =1, with kernel size of 5x5.')\n",
    "print('\\nlearning_rate = {}\\nbatch_size = {}\\nnum_epochs = {}\\n'.format(learning_rate, batch_size, num_epochs))\n",
    "\n",
    "run_model(mode=2, learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epochs)\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Model 3 ==================\n",
    "learning_rate = 0.03\n",
    "batch_size = 10\n",
    "num_epochs = 60\n",
    "\n",
    "print(('='*32)+' Training model 3 '+('='*32))\n",
    "print('Model 2 + replace Sigmoid with ReLU with new learning rate')\n",
    "print('\\nlearning_rate = {}\\nbatch_size = {}\\nnum_epochs = {}\\n'.format(learning_rate, batch_size, num_epochs))\n",
    "\n",
    "run_model(mode=3, learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epochs)\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Model 4 ==================\n",
    "learning_rate = 0.03\n",
    "batch_size = 10\n",
    "num_epochs = 60\n",
    "\n",
    "print(('='*32)+' Training model 4 '+('='*32))\n",
    "print('Model 3 + another fully connected (FC) layer (with 100 neurons)')\n",
    "print('\\nlearning_rate = {}\\nbatch_size = {}\\nnum_epochs = {}\\n'.format(learning_rate, batch_size, num_epochs))\n",
    "\n",
    "run_model(mode=4, learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epochs)\n",
    "print('='*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================== Model 5 ==================\n",
    "learning_rate = 0.03\n",
    "batch_size = 10\n",
    "num_epochs = 40\n",
    "\n",
    "print(('='*32)+' Training model 5 '+('='*32))\n",
    "print('Model 4 + Changed the neurons numbers in FC layers into 1000 with Dropout (with a rate of 0.5).')\n",
    "print('\\nlearning_rate = {}\\nbatch_size = {}\\nnum_epochs = {}\\n'.format(learning_rate, batch_size, num_epochs))\n",
    "\n",
    "run_model(mode=5, learning_rate=learning_rate, batch_size=batch_size, num_epochs=num_epochs)\n",
    "print('='*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
